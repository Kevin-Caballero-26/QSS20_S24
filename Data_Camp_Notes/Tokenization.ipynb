{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb790fb-4f06-4db2-bb3f-d4b241e4d1c8",
   "metadata": {},
   "source": [
    "# Tokenization and lemmatization\n",
    "\n",
    "* Make text friendly for machines\n",
    "* Dogs and dog are different words yet should be procossed as the same\n",
    "* Dont and do not etc etc\n",
    "\n",
    "# Common text preprocessing techniques\n",
    "\n",
    "* converting words into lowercase\n",
    "* removing leading and trailing whitespaces\n",
    "* removing punctuation\n",
    "* removing stop words\n",
    "* expanding contractions\n",
    "* removing special charachters\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "* Is the process of getting all the tokens into a list\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "* convert word into its base form\n",
    "* Ex: reducing, reduces, reduced, reduction become reduce\n",
    "* Ex: am, are, is to be\n",
    "* Ex: 've becomes have\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012a459e-17d7-40ee-8167-f07fdf0f9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# this is a small language model in the english language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "string = \"I have a dog. His name is Hachi.\"\n",
    "\n",
    "# seems to create tokens for every word in our string\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea53e7cd-3771-4f4a-8471-577fb1bf0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'dog', '.', 'his', 'name', 'be', 'Hachi', '.'] ['I', 'have', 'a', 'dog', '.', 'His', 'name', 'is', 'Hachi', '.']\n"
     ]
    }
   ],
   "source": [
    "# creates a list of the string from our tokens in our doc using list comprehension\n",
    "tokens = [token.text for token in doc] # extracts the text\n",
    "\n",
    "# Creates a list of the lemmatization\n",
    "\n",
    "# lemma is the base of the word\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(lemmas, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d7562cf-9bd2-4248-96ed-ab9539e85523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# string.isalpha() will return true if all charachters are letters and false otherwise\n",
    "# tokens like numbers, puctuatuon and emojis will not return true\n",
    "print(\"kevin\".isalpha())\n",
    "\n",
    "print(\"3kevin\".isalpha())\n",
    "\n",
    "# is alpha() is great but it is best to make our own functions to clean data\n",
    "# sometimes we need words that include other punctuation rather than just charachters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa06da8-314e-40fc-827f-164448db42e2",
   "metadata": {},
   "source": [
    "# Part of speech tagging\n",
    "\n",
    "* POS tagging\n",
    "* word sense disambiguation\n",
    "* Ex: the bear is a magestic animal vs please bear with me.\n",
    "* Clearly different but how will computer know?\n",
    "\n",
    "* sentiment analysis\n",
    "* question answering\n",
    "* Fake news and opinion spam detection\n",
    "\n",
    "What is it?\n",
    "\n",
    "* Assinging every word, its correspoinding part of speech\n",
    "* Jane is an amazing quitarist\n",
    "* propernoun, verb, determiner, adjecive, noun\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138c1335-e1d7-4479-9027-ed6316497169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jane', 'PROPN'),\n",
       " ('is', 'AUX'),\n",
       " ('an', 'DET'),\n",
       " ('amazing', 'ADJ'),\n",
       " ('guitarist', 'NOUN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Jane is an amazing guitarist\" \n",
    "\n",
    "# can identify about 20 parts of speach\n",
    "doc = nlp(string) # Creates our doc from nlp wich includes POS tagging in its data\n",
    "\n",
    "pos = [(token.text, token.pos_) for token in doc ]\n",
    "\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82fa48-e7ab-4f19-9270-76a062c1204e",
   "metadata": {},
   "source": [
    "# Name entity recognition\n",
    "\n",
    "* Efficient search algotithms\n",
    "* question answering\n",
    "\n",
    "Named entity recognition\n",
    "\n",
    "* Identifying and classyfying entities into predefined categories\n",
    "* categories include person, organizatin, country, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d378d16-81a0-4572-b08d-7b56deeefa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John Doe', 'PERSON'),\n",
       " ('Google', 'ORG'),\n",
       " ('France', 'GPE'),\n",
       " ('Angel', 'PERSON')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"John Doe is a software engineer working at Google. He lives in France and has a dog named Angel.\"\n",
    "\n",
    "doc = nlp(string)\n",
    "\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "ne\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
